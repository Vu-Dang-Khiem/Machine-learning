import streamlit as st
import pickle
import re, regex
from underthesea import word_tokenize

# Load model
tfidf = pickle.load(open("tfidf.pkl", "rb"))
model = pickle.load(open("model_svm.pkl", "rb"))

# Stopwords
VIETNAMESE_STOPWORDS = {
    "l√†","v√†","c·ªßa","c√≥","cho","m·ªôt","nh·ªØng","c√°c",
    "ƒë√£","ƒëang","s·∫Ω","n√†y","ƒë√≥","v·ªõi","khi","t·∫°i",
    "theo","ƒë·∫øn","t·ª´","v·ªÅ","trong","ra","nh∆∞"
}
NEGATION_WORDS = {"kh√¥ng","ch∆∞a","ch·∫≥ng"}

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+", " ", text)
    text = re.sub(r"\S+@\S+", " ", text)
    text = re.sub(r"\d+", " ", text)
    text = regex.sub(r"[^\p{L}\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    tokens = word_tokenize(text, format="text").split()
    tokens = [w for w in tokens if (w not in VIETNAMESE_STOPWORDS) or (w in NEGATION_WORDS)]
    return " ".join(tokens)

# UI
st.set_page_config(page_title="Fake News Detection", layout="centered")

st.title("üì∞ Fake News Detection")
st.write("Nh·∫≠p n·ªôi dung b√†i b√°o ti·∫øng Vi·ªát ƒë·ªÉ ki·ªÉm tra")

text_input = st.text_area("üìÑ N·ªôi dung b√†i b√°o", height=250)

if st.button("üîç Ki·ªÉm tra"):
    if text_input.strip() == "":
        st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p n·ªôi dung")
    else:
        clean = clean_text(text_input)
        X = tfidf.transform([clean])
        pred = model.predict(X)[0]

        if pred == 1:
            st.error("üö® K·∫æT QU·∫¢: FAKE NEWS")
        else:
            st.success("‚úÖ K·∫æT QU·∫¢: REAL NEWS")

        st.markdown("---")
        st.write("**VƒÉn b·∫£n sau khi l√†m s·∫°ch:**")
        st.code(clean)
